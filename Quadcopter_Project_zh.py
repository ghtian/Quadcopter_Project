#!/usr/bin/env python
# coding: utf-8

# # é¡¹ç›®ï¼šæŒ‡å¯¼å››è½´é£è¡Œå™¨å­¦ä¼šé£è¡Œ
# 
# è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä½¿å››è½´é£è¡Œå™¨é£è¡Œçš„æ™ºèƒ½ä½“ï¼Œç„¶åä½¿ç”¨ä½ é€‰æ‹©çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒå®ƒï¼
# 
# è¯·å°è¯•è¿ç”¨ä½ åœ¨è¿™ä¸€å•å…ƒä¸­å­¦åˆ°çš„çŸ¥è¯†ï¼Œçœ‹çœ‹å“ªä¸ªæ–¹æ³•æ•ˆæœæœ€å¥½ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥è‡ªå·±æƒ³å‡ºåˆ›æ–°å‹æ–¹æ³•å¹¶æµ‹è¯•å®ƒä»¬ã€‚
# ## è¯´æ˜
# 
# è¯·æŸ¥çœ‹ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œä»¥æ›´å¥½åœ°äº†è§£é¡¹ç›®ç»“æ„ã€‚ 
# 
# - `task.py`ï¼šåœ¨æœ¬æ–‡ä»¶ä¸­å®šä¹‰ä½ çš„ä»»åŠ¡ï¼ˆç¯å¢ƒï¼‰ã€‚
# - `agents/`ï¼šæœ¬æ–‡ä»¶å¤¹ä¸­åŒ…å«å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚
#     - `policy_search.py`ï¼šæˆ‘ä»¬ä¸ºä½ æä¾›äº†ä¸€ä¸ªæ™ºèƒ½ä½“æ¨¡æ¿ã€‚
#     - `agent.py`ï¼šåœ¨æœ¬æ–‡ä»¶ä¸­å¼€å‘ä½ çš„æ™ºèƒ½ä½“ã€‚
# - `physics_sim.py`ï¼šæœ¬æ–‡ä»¶ä¸­åŒ…å«å››è½´é£è¡Œå™¨æ¨¡æ‹Ÿå™¨ã€‚**è¯·å‹¿ä¿®æ”¹æœ¬æ–‡ä»¶**ã€‚
# 
# åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œä½ éœ€è¦åœ¨ `task.py` ä¸­å®šä¹‰ä½ çš„ä»»åŠ¡ã€‚å°½ç®¡æˆ‘ä»¬ä¸ºä½ æä¾›äº†ä¸€ä¸ªä»»åŠ¡ç¤ºä¾‹ï¼Œæ¥å¸®åŠ©ä½ å¼€å§‹é¡¹ç›®ï¼Œä½†ä½ ä¹Ÿå¯ä»¥éšæ„æ›´æ”¹è¿™ä¸ªæ–‡ä»¶ã€‚åœ¨è¿™ä¸ª notebook ä¸­ï¼Œä½ è¿˜å°†å­¦ä¹ æ›´å¤šæœ‰å…³ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶çš„çŸ¥è¯†ã€‚
# 
# ä½ è¿˜éœ€è¦åœ¨ `agent.py` ä¸­è®¾è®¡ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæ¥å®Œæˆä½ é€‰æ‹©çš„ä»»åŠ¡ã€‚
# 
# æˆ‘ä»¬ä¹Ÿé¼“åŠ±ä½ åˆ›å»ºå…¶ä»–æ–‡ä»¶ï¼Œæ¥å¸®åŠ©ä½ æ•´ç†ä»£ç ã€‚æ¯”å¦‚ï¼Œä½ ä¹Ÿè®¸å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ª `model.py` æ–‡ä»¶æ¥å®šä¹‰å…¶ä»–ä½ éœ€è¦çš„ç¥ç»ç½‘ç»œç»“æ„ã€‚
# 
# ## æ§åˆ¶å››è½´é£è¡Œå™¨
# 
# åœ¨ä¸‹æ–¹çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ™ºèƒ½ä½“ç¤ºä¾‹ï¼Œæ¥ç¤ºèŒƒå¦‚ä½•ä½¿ç”¨æ¨¡æ‹Ÿå™¨æ¥æ§åˆ¶å››è½´é£è¡Œå™¨ã€‚è¿™ä¸ªæ™ºèƒ½ä½“æ¯”ä½ åœ¨ notebook ä¸­éœ€è¦æµ‹è¯•çš„æ™ºèƒ½ä½“ï¼ˆåœ¨ `agents/policy_search.py` ä¸­ï¼‰æ›´åŠ ç®€å•ï¼
# 
# è¿™ä¸ªæ™ºèƒ½ä½“é€šè¿‡è®¾ç½®é£è¡Œå™¨å››ä¸ªè½´ä¸Šçš„è½¬é€Ÿæ¥æ§åˆ¶é£è¡Œå™¨ã€‚`Basic_Agent` ç±»ä¸­æä¾›çš„æ™ºèƒ½ä½“å°†ä¼šéšæœºä¸ºå››ä¸ªè½´æŒ‡å®šåŠ¨ä½œã€‚è¿™å››ä¸ªé€Ÿåº¦å°†é€šè¿‡ `act` æ–¹æ³•ä»¥å››ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨çš„å½¢å¼è¿”å›ã€‚
# 
# åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œä½ å°†åœ¨ `agents/agent.py` ä¸­å®ç°çš„æ™ºèƒ½ä½“ä¼šä»¥æ›´åŠ æ™ºèƒ½çš„æ–¹æ³•è¿›è¡ŒæŒ‡å®šçš„åŠ¨ä½œã€‚

# ### plot

# In[1]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

def plot_run(results, standalone=True):
    if standalone:
        plt.subplots(figsize=(15, 15))
    
    #æŸ¥çœ‹å››è½´é£è¡Œå™¨çš„ä½ç½®å˜åŒ–
    plt.subplot(3, 3, 1)
    plt.title('Position')
    plt.plot(results['time'], results['x'], label='x')
    plt.plot(results['time'], results['y'], label='y')
    plt.plot(results['time'], results['z'], label='z')
    plt.xlabel('time, seconds')
    plt.ylabel('Position')
    plt.grid(True)
    if standalone:
        plt.legend()

    #å››è½´é£è¡Œå™¨çš„é€Ÿåº¦
    plt.subplot(3, 3, 2)
    plt.title('Velocity')
    plt.plot(results['time'], results['x_velocity'], label='x_hat')
    plt.plot(results['time'], results['y_velocity'], label='y_hat')
    plt.plot(results['time'], results['z_velocity'], label='z_hat')
    plt.xlabel('time, seconds')
    plt.ylabel('Velocity')
    plt.grid(True)
    if standalone:
        plt.legend()
    
    #ç»˜åˆ¶æ¬§æ‹‰è§’ (Euler angles)ï¼ˆå››è½´é£è¡Œå™¨å›´ç»• xï¼Œy å’Œ z è½´çš„æ—‹è½¬ï¼‰çš„å›¾è¡¨
    plt.subplot(3, 3, 3)
    plt.title('Orientation')
    plt.plot(results['time'], results['phi'], label='phi')
    plt.plot(results['time'], results['theta'], label='theta')
    plt.plot(results['time'], results['psi'], label='psi')
    plt.xlabel('time, seconds')
    plt.grid(True)
    if standalone:
        plt.legend()
    
    #ç»˜åˆ¶æ¯ä¸ªæ¬§æ‹‰è§’çš„é€Ÿåº¦ï¼ˆæ¯ç§’çš„å¼§åº¦ï¼‰å›¾
    plt.subplot(3, 3, 4)
    plt.title('Angular Velocity')
    plt.plot(results['time'], results['phi_velocity'], label='phi')
    plt.plot(results['time'], results['theta_velocity'], label='theta')
    plt.plot(results['time'], results['psi_velocity'], label='psi')
    plt.xlabel('time, seconds')
    plt.grid(True)
    if standalone:
        plt.legend()

    #æœ€åï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹æ–¹ä»£ç æ¥è¾“å‡ºæ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œã€‚
    plt.subplot(3, 3, 5)
    plt.title('Rotor Speed')
    plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1')
    plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2')
    plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3')
    plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4')
    plt.xlabel('time, seconds')
    plt.ylabel('Rotor Speed, revolutions / second')
    plt.grid(True)
    if standalone:
        plt.legend()

    if standalone:
        plt.tight_layout()
        plt.show()


# In[2]:


from mpl_toolkits.mplot3d.axes3d import Axes3D


def plot_point3d(ax, x, y, z, **kwargs):
    ax.scatter([x], [y], [z], **kwargs)
    ax.text(x, y, z, "({:.1f}, {:.1f}, {:.1f})".format(x, y, z))


def show_flight_path(results, target=None):
    results = np.array(results)
    
    fig = plt.figure(figsize=(10,10))
    ax = fig.gca(projection='3d')
    ax.set_xlabel('X-axis')
    ax.set_ylabel('Y-axis')
    ax.set_zlabel('Z-axis')

    ax.plot3D(results[:, 0], results[:, 1], results[:, 2], 'gray')
    
    if target is not None:
        plot_point3d(ax, *target[0:3], c='y', marker='x', s=100, label='target')
        
    plot_point3d(ax, *results[0, 0:3], c='g', marker='o', s=50, label='start')
    plot_point3d(ax, *results[-1, 0:3], c='r', marker='o', s=50, label='end')
    
    ax.legend()


# ### random Agent

# In[3]:


import random

class Basic_Agent():
    def __init__(self, task):
        self.task = task
    
    def act(self):
        new_thrust = random.gauss(450., 25.)
        return [new_thrust + random.gauss(0., 1.) for x in range(4)]


# è¿è¡Œä¸‹æ–¹ä»£ç ï¼Œè®©æ™ºèƒ½ä½“æŒ‡å®šåŠ¨ä½œæ¥æ§åˆ¶å››è½´é£è¡Œå™¨ã€‚
# 
# è¯·éšæ„æ›´æ”¹æˆ‘ä»¬æä¾›çš„ `runtime`ï¼Œ`init_pose`ï¼Œ`init_velocities` å’Œ `init_angle_velocities` å€¼æ¥æ›´æ”¹å››è½´é£è¡Œå™¨çš„åˆå§‹æ¡ä»¶ã€‚
# 
# ä¸‹æ–¹çš„ `labels` åˆ—è¡¨ä¸ºæ¨¡æ‹Ÿæ•°æ®çš„æ³¨é‡Šã€‚æ‰€æœ‰çš„ä¿¡æ¯éƒ½å‚¨å­˜åœ¨ `data.txt` æ–‡æ¡£ä¸­ï¼Œå¹¶ä¿å­˜åœ¨ `results` ç›®å½•ä¸‹ã€‚

# In[4]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')

import csv
import numpy as np
from task import Task

# Modify the values below to give the quadcopter a different starting position.
runtime = 5.                                     # time limit of the episode
init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose
init_velocities = np.array([0., 0., 0.])         # initial velocities
init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities
file_output = 'data.txt'                         # file name for saved results

# Setup
task = Task(init_pose, init_velocities, init_angle_velocities, runtime)
agent = Basic_Agent(task)
done = False
labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',
          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',
          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']
results = {x : [] for x in labels}

# Run the simulation, and save the results.
with open(file_output, 'w') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(labels)
    while True:
        rotor_speeds = agent.act()
        _, _, done = task.step(rotor_speeds)
        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)
        for ii in range(len(labels)):
            results[labels[ii]].append(to_write[ii])
        writer.writerow(to_write)
        if done:
            break


# In[5]:


plot_run(results)


# In[6]:


path = [[results['x'][i], results['y'][i], results['z'][i]] for i in range(len(results['x']))]
show_flight_path(path, target=None)


# åœ¨æŒ‡å®šä»»åŠ¡ä¹‹å‰ï¼Œä½ éœ€è¦åœ¨æ¨¡æ‹Ÿå™¨ä¸­è¡ç”Ÿç¯å¢ƒçŠ¶æ€ã€‚è¿è¡Œä¸‹æ–¹ä»£ç æ¥åœ¨æ¨¡æ‹Ÿç»“æŸæ—¶è¾“å‡ºä»¥ä¸‹å˜é‡å€¼ï¼š
# 
# - `task.sim.pose`ï¼šå››å‘¨é£è¡Œå™¨åœ¨ ($x,y,z$) åæ ‡ç³»ä¸­çš„ä½ç½®å’Œæ¬§æ‹‰è§’ã€‚
# - `task.sim.v`ï¼šå››è½´é£è¡Œå™¨åœ¨ ($x,y,z$) åæ ‡ç³»ä¸­çš„é€Ÿåº¦ã€‚
# - `task.sim.angular_v`ï¼šä¸‰ä¸ªæ¬§æ‹‰è§’çš„å¼§åº¦/æ¯ç§’ã€‚

# In[7]:


# the pose, velocity, and angular velocity of the quadcopter at the end of the episode
print(task.sim.pose)
print(task.sim.v)
print(task.sim.angular_v)


# åœ¨ `task.py` ä¸­çš„ä»»åŠ¡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å››è½´é£è¡Œå™¨å…­ä¸ªç»´åº¦çš„åŠ¨ä½œæ¥æ„å»ºæ¯ä¸ªæ—¶é—´æ­¥çš„ç¯å¢ƒçŠ¶æ€ã€‚ç„¶è€Œï¼Œä½ ä¹Ÿå¯ä»¥æŒ‰ç…§è‡ªå·±çš„æ„æ„¿æ›´æ”¹ä»»åŠ¡ï¼Œä½ å¯ä»¥æ·»åŠ é€Ÿåº¦ä¿¡æ¯æ¥æ‰©å¤§çŠ¶æ€å‘é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•åŠ¨ä½œã€é€Ÿåº¦å’Œè§’é€Ÿåº¦çš„ç»„åˆï¼Œå¹¶æ„é€ é€‚ç”¨äºä½ çš„ä»»åŠ¡çš„ç¯å¢ƒçŠ¶æ€ã€‚
# 
# ## ä»»åŠ¡
# 
# åœ¨ `task.py` ä¸­ï¼Œæˆ‘ä»¬ä¸ºä½ æä¾›äº†ä¸€ä¸ªä»»åŠ¡ç¤ºä¾‹ã€‚è¯·åœ¨æ–°çª—å£ä¸­æ‰“å¼€è¿™ä¸ªæ–‡ä»¶ã€‚
# 
# ä½¿ç”¨ `__init__()` æ–¹æ³•æ¥åˆå§‹åŒ–æŒ‡å®šæœ¬ä»»åŠ¡æ‰€éœ€çš„å‡ ä¸ªå˜é‡ã€‚
# 
# - æ¨¡æ‹Ÿå™¨ä½œä¸º `PhysicsSim` ç±»ï¼ˆæ¥è‡ª `physics_sim.py` æ–‡ä»¶ï¼‰çš„ç¤ºä¾‹è¿›è¡Œåˆå§‹åŒ–ã€‚
# - å—åˆ° DDPG è®ºæ–‡ä¸­ç ”ç©¶æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é‡å¤è°ƒç”¨åŠ¨ä½œçš„æ–¹æ³•ã€‚å¯¹äºæ™ºèƒ½ä½“çš„æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ `action_repeats` æ—¶é—´æ­¥æ¥è¿›è¡Œæ¨¡æ‹Ÿã€‚å¦‚æœä½ å¹¶ä¸ç†Ÿæ‚‰è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥é˜…è¯» [DDPG è®ºæ–‡](https://arxiv.org/abs/1509.02971)çš„ç»“è®ºéƒ¨åˆ†ã€‚
# - æˆ‘ä»¬è®¾ç½®äº†çŠ¶æ€å‘é‡ä¸­æ¯ä¸ªåˆ†é‡çš„æ•°å€¼ã€‚åœ¨ä»»åŠ¡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åªè®¾ç½®äº†å…­ä¸ªç»´åº¦çš„åŠ¨ä½œä¿¡æ¯ã€‚ä¸ºäº†è®¾å®šå‘é‡å¤§å°ï¼ˆ`state_size`ï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘é‡å¤çš„åŠ¨ä½œã€‚
# - ä»»åŠ¡ç¯å¢ƒé€šå¸¸æ˜¯ä¸€ä¸ªå››ç»´åŠ¨ä½œç©ºé—´ï¼Œæ¯ä¸ªè½´æœ‰ä¸€ä¸ªè¾“å…¥ï¼ˆ`action_size=4`ï¼‰ã€‚ä½ å¯ä»¥è®¾ç½®æ¯ä¸ªè¾“å…¥çš„æœ€å°å€¼ï¼ˆ`action_low`ï¼‰å’Œæœ€å¤§å€¼ï¼ˆ`action_high`ï¼‰ã€‚
# - æˆ‘ä»¬åœ¨æ–‡ä»¶ä¸­æä¾›çš„ä»»åŠ¡ç¤ºä¾‹å°†ä½¿æ™ºèƒ½ä½“è¾¾åˆ°ç›®æ ‡ä½ç½®ã€‚æˆ‘ä»¬å°†ç›®æ ‡ä½ç½®è®¾ç½®ä¸ºä¸€ä¸ªå˜é‡ã€‚
# 
# `reset()` æ–¹æ³•å°†é‡ç½®æ¨¡æ‹Ÿå™¨ã€‚æ¯å½“é˜¶æ®µç»“æŸæ—¶ï¼Œæ™ºèƒ½ä½“éƒ½å°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚ä½ å¯ä»¥æŸ¥çœ‹ä¸‹æ–¹ä»£ç ä¸­çš„ä¾‹å­ã€‚
# 
# `step()` æ–¹æ³•æ˜¯æœ€é‡è¦çš„ä¸€ä¸ªæ–¹æ³•ã€‚å®ƒå°†æ¥æ”¶æ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œ `rotor_speeds`ï¼Œå¹¶å‡†å¤‡å¥½ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ŒåŒæ—¶è¿”å›ç»™æ™ºèƒ½ä½“ã€‚æ¥ç€ï¼Œä½ å°†é€šè¿‡ `get_reward()` è®¡ç®—å¥–åŠ±å€¼ã€‚å½“è¶…è¿‡è§„å®šæ—¶é—´ï¼Œæˆ–æ˜¯å››è½´é£è¡Œå™¨åˆ°è¾¾æ¨¡æ‹Ÿå™¨è¾¹ç¼˜æ—¶ï¼Œè¿™ä¸€é˜¶æ®µå°†è§†ä½œç»“æŸã€‚
# 
# æ¥ä¸‹æ¥ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•æµ‹è¯•è¿™ä¸ªä»»åŠ¡ä¸­æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚
# 
# ## æ™ºèƒ½ä½“
# 
# `agents/policy_search.py` æ–‡ä»¶ä¸­æä¾›çš„æ™ºèƒ½ä½“ç¤ºä¾‹ä½¿ç”¨äº†éå¸¸ç®€å•çš„çº¿æ€§ç­–ç•¥ï¼Œå°†åŠ¨ä½œå‘é‡è§†ä½œçŠ¶æ€å‘é‡å’ŒçŸ©é˜µæƒé‡çš„ç‚¹ç§¯ç›´æ¥è¿›è¡Œè®¡ç®—ã€‚æ¥ç€ï¼Œå®ƒé€šè¿‡æ·»åŠ ä¸€äº›é«˜æ–¯å™ªå£°æ¥éšæœºå¹²æ‰°å‚æ•°ï¼Œä»¥äº§ç”Ÿä¸åŒçš„ç­–ç•¥ã€‚æ ¹æ®æ¯ä¸ªé˜¶æ®µè·å¾—çš„å¹³å‡å¥–åŠ±å€¼ï¼ˆ`score`ï¼‰ï¼Œå®ƒå°†è®°å½•è¿„ä»Šä¸ºæ­¢å‘ç°çš„æœ€ä½³å‚æ•°é›†ä»¥åŠåˆ†æ•°çš„å˜åŒ–çŠ¶æ€ï¼Œå¹¶æ®æ­¤è°ƒæ•´æ¯”ä¾‹å› å­æ¥æ‰©å¤§æˆ–å‡å°‘å™ªéŸ³ã€‚
# 
# è¯·è¿è¡Œä¸‹æ–¹ä»£ç æ¥æŸ¥çœ‹ä»»åŠ¡ç¤ºä¾‹ä¸­æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚

# In[8]:


import sys
import pandas as p


# In[9]:


from agents.policy_search import PolicySearch_Agent
from task import Task

labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',
          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',
          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']
results = {x : [] for x in labels}

num_episodes = 2
target_pos = np.array([0., 0., 10.])
task = Task(target_pos=target_pos)
agent = PolicySearch_Agent(task) 

for i_episode in range(1, num_episodes+1):
    state = agent.reset_episode() # start a new episode
    while True:
        action = agent.act(state) 
        next_state, reward, done = task.step(action)
        if i_episode == num_episodes:
            to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(action)
            for ii in range(len(labels)):
                results[labels[ii]].append(to_write[ii])
        agent.step(reward, done)
        state = next_state
        if done:
            print("\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}".format(
                i_episode, agent.score, agent.best_score, agent.noise_scale), end="")  # [debug]
            break
    sys.stdout.flush()


# In[10]:


plot_run(results)

path = [[results['x'][i], results['y'][i], results['z'][i]] for i in range(len(results['x']))]
show_flight_path(path, target=target_pos)


# è¿™ä¸ªæ™ºèƒ½ä½“çš„æ€§èƒ½æƒ³å¿…ååˆ†ç³Ÿç³•ï¼ç°åœ¨è½®åˆ°ä½ å‡ºåœºäº†ï¼
# 
# ## å®šä¹‰ä»»åŠ¡ï¼Œè®¾è®¡å¹¶è®­ç»ƒä½ çš„æ™ºèƒ½ä½“ï¼
# 
# ä¿®æ”¹ `task.py` æ–‡ä»¶æ¥æŒ‡å®šä½ æ‰€é€‰æ‹©çš„ä»»åŠ¡ã€‚å¦‚æœä½ ä¸ç¡®å®šé€‰æ‹©ä»€ä¹ˆä»»åŠ¡ï¼Œä½ å¯ä»¥æ•™ä½ çš„å››è½´é£è¡Œå™¨èµ·é£ã€ç›˜æ—‹ã€ç€é™†æˆ–æ˜¯è¾¾åˆ°æŒ‡å®šä½ç½®ã€‚
# 
# 
# åœ¨æŒ‡å®šä»»åŠ¡åï¼Œä½¿ç”¨ `agents/policy_search.py` ä¸­çš„æ™ºèƒ½ä½“ç¤ºä¾‹ä½œä¸ºæ¨¡æ¿ï¼Œæ¥åœ¨ `agents/agent.py` ä¸­å®šä¹‰ä½ è‡ªå·±çš„æ™ºèƒ½ä½“ã€‚ä½ å¯ä»¥éšæ„ä»æ™ºèƒ½ä½“ç¤ºä¾‹ä¸­å€Ÿç”¨ä½ éœ€è¦çš„å…ƒç´ ï¼ŒåŒ…æ‹¬å¦‚ä½•æ¨¡å—åŒ–ä½ çš„ä»£ç ï¼ˆä½¿ç”¨ `act()`ï¼Œ`learn()` å’Œ `reset_episode_vars()` ç­‰è¾…åŠ©æ–¹æ³•ï¼‰ã€‚
# 
# è¯·æ³¨æ„ï¼Œä½ æŒ‡å®šçš„ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“å’Œä»»åŠ¡**ææœ‰å¯èƒ½**æ— æ³•é¡ºåˆ©è¿›è¡Œå­¦ä¹ ã€‚ä½ å°†éœ€è¦æ”¹è¿›ä¸åŒçš„è¶…å‚æ•°å’Œå¥–åŠ±å‡½æ•°ï¼Œç›´åˆ°ä½ èƒ½å¤Ÿè·å¾—ä¸é”™çš„ç»“æœã€‚
# 
# åœ¨å¼€å‘æ™ºèƒ½ä½“çš„æ—¶å€™ï¼Œä½ è¿˜éœ€è¦å…³æ³¨å®ƒçš„æ€§èƒ½ã€‚å‚è€ƒä¸‹æ–¹ä»£ç ï¼Œå»ºç«‹ä¸€ä¸ªæœºåˆ¶æ¥å­˜å‚¨æ¯ä¸ªé˜¶æ®µçš„æ€»å¥–åŠ±å€¼ã€‚å¦‚æœé˜¶æ®µå¥–åŠ±å€¼åœ¨é€æ¸ä¸Šå‡ï¼Œè¯´æ˜ä½ çš„æ™ºèƒ½ä½“æ­£åœ¨å­¦ä¹ ã€‚

# In[97]:


## TODO: Train your agent here.
##tim's work
from agents.agent import Agent
from agents.myagent import DDPG
from agenttask import AgentTask

labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',
          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',
          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']
results = {x : [] for x in labels}

num_episodes = 2000
init_pose = np.array([0., 0., 0., 0., 0., 0.])
target_pos = np.array([0., 0., 10.])
task = AgentTask(init_pose=init_pose, target_pos=target_pos)
#agent = Agent(task) 
agent = DDPG(task) 
list_reward = []

for i_episode in range(1, num_episodes+1):
    state = agent.reset_episode() # start a new episode
    episode_reward = 0.0
    while True:
        action = agent.act(state) 
        next_state, reward, done = task.step(action)
        episode_reward += reward
        if i_episode == num_episodes:
            to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(action)
            for ii in range(len(labels)):
                results[labels[ii]].append(to_write[ii])
        agent.step(action, reward, next_state, done)
        state = next_state
        
        if done:
            list_reward.append(episode_reward)
            print("\rEpisode = {:4d}, score = {:7.3f} (pose = {:7.3f}), velocity = {}".format(
                i_episode, episode_reward, task.sim.pose[2], task.sim.v[2]), end="")  # ä»»åŠ¡è®¾è®¡ä¸ºèµ·é£ï¼Œå› æ­¤å…³æ³¨åœ¨Zè½´ä¸Šçš„ä½ç½®ä¸é€Ÿåº¦
            break
    sys.stdout.flush()


# In[98]:


#plot_run(results)

path = [[results['x'][i], results['y'][i], results['z'][i]] for i in range(len(results['x']))]
show_flight_path(path, target=target_pos)


# ## ç»˜åˆ¶é˜¶æ®µå¥–åŠ±
# 
# è¯·ç»˜åˆ¶æ™ºèƒ½ä½“åœ¨æ¯ä¸ªé˜¶æ®µä¸­è·å¾—çš„æ€»å¥–åŠ±ï¼Œè¿™å¯ä»¥æ˜¯å•æ¬¡è¿è¡Œçš„å¥–åŠ±å€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šæ¬¡è¿è¡Œçš„å¹³å‡å€¼ã€‚

# In[112]:


## TODO: Plot the rewards.
plt.title("The whole view")
plt.plot(list_reward, label='episode_reward')
plt.legend()


# In[114]:


plt.title("some episodes")
plt.plot(list_reward[300:500], label='episode_reward')
plt.legend()


# In[115]:


#æ‰“å°åé˜¶æ®µçš„å¹³å‡å€¼
episode_last = 10
print("The average reward of last {} episodes is {:4.3f}!".format(
episode_last,
np.sum(list_reward[-episode_last:])/episode_last
))


# ## å›é¡¾
# 
# **é—®é¢˜ 1**ï¼šè¯·æè¿°ä½ åœ¨ `task.py` ä¸­æŒ‡å®šçš„ä»»åŠ¡ã€‚ä½ å¦‚ä½•è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Ÿ
# 
# **å›ç­”**ï¼š
# - ä»»åŠ¡ï¼šå¼•å¯¼é£è¡Œå™¨èµ·é£ï¼Œèµ·ç‚¹æ˜¯[0., 0., 0.], ç»ˆç‚¹æ˜¯[0., 0., 10.]ï¼ŒæœŸæœ›é£è¡Œå™¨æ²¿Zè½´å‚ç›´ä¸Šå‡ã€‚
# - å¥–åŠ±å‡½æ•°ï¼šä»¥å½“å‰ä½ç½®ä¸ç›®æ ‡ä½ç½®çš„ä¸‰è½´åæ ‡å·®ä½œä¸ºåŸºç¡€ï¼Œæ‰§è¡Œæ±‚å’Œï¼ŒåŠ æƒä»¥é™ä½å½±å“ï¼Œå–å…¶ç›¸åæ•°ï¼Œé™„åŠ å€¼+1ã€‚ä½¿å¾—å·®è·è¶Šå°ï¼Œå¥–åŠ±è¶Šå¤§ã€‚
# 
# 
# **é—®é¢˜ 2**ï¼šè¯·ç®€è¦æè¿°ä½ çš„æ™ºèƒ½ä½“ï¼Œä½ å¯ä»¥å‚è€ƒä»¥ä¸‹é—®é¢˜ï¼š
# 
# - ä½ å°è¯•äº†å“ªäº›å­¦ä¹ ç®—æ³•ï¼Ÿå“ªä¸ªæ•ˆæœæœ€å¥½ï¼Ÿ
# - ä½ æœ€ç»ˆé€‰æ‹©äº†å“ªäº›è¶…å‚æ•°ï¼ˆæ¯”å¦‚ $\alpha$ï¼Œ$\gamma$ï¼Œ$\epsilon$ ç­‰ï¼‰ï¼Ÿ
# - ä½ ä½¿ç”¨äº†ä»€ä¹ˆæ ·çš„ç¥ç»ç½‘ç»œç»“æ„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Ÿè¯·è¯´æ˜å±‚æ•°ã€å¤§å°å’Œæ¿€æ´»å‡½æ•°ç­‰ä¿¡æ¯ã€‚
# 
# **å›ç­”**ï¼š
# - ä½¿ç”¨äº†Qå€¼ç®—æ³•ï¼ŒåŒæ—¶å¼•ç”¨äº†DDPGç®—æ³•ã€‚ä»æ™ºèƒ½ä½“çš„é£è¡Œè½¨è¿¹æ¥çœ‹ï¼Œä¼¼ä¹ç®—æ³•éƒ½æœªå¤Ÿç†æƒ³ã€‚
# - ğ›¾ï¼Œè®¾ç½®è¾ƒé«˜ï¼Œå‡ ä¹æ— è¡°å‡ã€‚åŒæ—¶åœ¨æ„å»ºDDPGæ™ºèƒ½ä½“æ—¶ç›´æ¥æŒ‡å®šäº†ä¸¤ä¸ªç¥ç»ç½‘ç»œçš„å­¦ä¹ ç‡ã€‚  
# - ä½¿ç”¨äº†ä¸€èˆ¬çš„ç¥ç»ç½‘ç»œï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
# 
#   --è¡ŒåŠ¨è€…ï¼šå…±6å±‚ï¼Œå„å±‚å¤§å°ä¸æ¿€æ´»å‡½æ•°ä¾æ¬¡ä¸º18 - 32(relu) - 64(relu) - 32(relu) - 4(sigmoid) - 4ã€‚  
#   --è¯„è®ºè€…ï¼šå…±6å±‚ï¼Œå‰3å±‚ä¸ºåˆ†æ”¯ç½‘ç»œï¼ŒçŠ¶æ€åˆ†æ”¯ä¸º18 - 32(relu) - 64(relu)ï¼ŒåŠ¨ä½œåˆ†æ”¯ä¸º4 - 32(relu) - 64(relu)ï¼Œç¬¬4å±‚çš„è¾“å…¥åˆå¹¶äº†ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºï¼Œå3å±‚çš„ç»“æ„ä¸º64 - 64(relu) - 1ã€‚
# 
# 
# **é—®é¢˜ 3**ï¼šæ ¹æ®ä½ ç»˜åˆ¶çš„å¥–åŠ±å›¾ï¼Œæè¿°æ™ºèƒ½ä½“çš„å­¦ä¹ çŠ¶å†µã€‚
# 
# - å­¦ä¹ è¯¥ä»»åŠ¡æ˜¯ç®€å•è¿˜æ˜¯å›°éš¾ï¼Ÿ
# - è¯¥å­¦ä¹ æ›²çº¿ä¸­æ˜¯å¦å­˜åœ¨å¾ªåºæ¸è¿›æˆ–æ€¥é€Ÿä¸Šå‡çš„éƒ¨åˆ†ï¼Ÿ
# - è¯¥æ™ºèƒ½ä½“çš„æœ€ç»ˆæ€§èƒ½æœ‰å¤šå¥½ï¼Ÿï¼ˆæ¯”å¦‚æœ€ååä¸ªé˜¶æ®µçš„å¹³å‡å¥–åŠ±å€¼ï¼‰
# 
# **å›ç­”**ï¼š
# - è¾ƒä¸ºå›°éš¾ï¼Œä¼¼ä¹æ— æ³•è¾ƒå¥½åœ°å¼•å¯¼æ™ºèƒ½ä½“é£å‘ç›®çš„åœ°
# - ä»æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹æ¥çœ‹ï¼Œå¾ªåºæ¸è¿›çš„é˜¶æ®µè¾ƒå°‘ï¼Œéƒ¨åˆ†é˜¶æ®µæ€¥é€Ÿä¸Šå‡ã€‚
# - æ•ˆæœä¸ç†æƒ³ï¼Œè¶Šæ¥è¶Šè¿œç¦»ç›®æ ‡ã€‚æœ€ååä¸ªé˜¶æ®µçš„å¹³å‡å¥–åŠ±æ˜¯20.341ã€‚
# 
# 
# **é—®é¢˜ 4**ï¼šè¯·ç®€è¦æ€»ç»“ä½ çš„æœ¬æ¬¡é¡¹ç›®ç»å†ã€‚ä½ å¯ä»¥å‚è€ƒä»¥ä¸‹é—®é¢˜ï¼š
# 
# - æœ¬æ¬¡é¡¹ç›®ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚å¼€å§‹é¡¹ç›®ã€è¿è¡Œ ROSã€ç»˜åˆ¶ã€ç‰¹å®šçš„ä»»åŠ¡ç­‰ã€‚ï¼‰
# - å…³äºå››è½´é£è¡Œå™¨å’Œä½ çš„æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä½ æ˜¯å¦æœ‰ä¸€äº›æœ‰è¶£çš„å‘ç°ï¼Ÿ
# 
# **å›ç­”**ï¼š
# - è™½å·²äº†è§£å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¨¡å‹ï¼Œä½†å°†å¿ƒä¸­çš„æŠ½è±¡æ¨¡å‹ä¸ç¨‹åºä»£ç å¯¹åº”çš„è¿‡ç¨‹ï¼Œè¿˜æ˜¯èŠ±äº†ä¸å°‘æ—¶é—´ï¼›å¥–åŠ±å‡½æ•°æ¯”è¾ƒéš¾æŠŠæ¡ã€‚
# - å‘ç°å¹¶é‡æ–°å­¦ä¹ äº†æ¬§æ‹‰è§’è¿™ä¸€æ–¹ä½å½¢æ€æ¦‚å¿µï¼Œæ­¤åº”ä¸ºé£è¡Œé™€èºä»ªçš„æ ¸å¿ƒæ„ä¹‰æ‰€åœ¨ã€‚
# - åœ¨è®¾è®¡å¥–åŠ±å‡½æ•°æ—¶ï¼Œæˆ‘å°½é‡ä»¤å¤§éƒ¨åˆ†ä¸æœŸæœ›å‡ºç°çš„çŠ¶æ€ä¹‹å¥–åŠ±è¾ƒä½ï¼Œç”šè‡³ä¸ºè´Ÿå€¼ï¼Œå› æ­¤è°ƒæ•´äº†é™„åŠ å€¼ä»¥è¾¾æ­¤ç›®çš„ã€‚
# - é™„åŠ å¿ƒå¾—ï¼šè§£å†³é—®é¢˜æ—¶ï¼Œäº†è§£ç¨‹åºæ¡†æ¶è¿™ä¸€åŸºç¡€æ­¥éª¤ï¼Œä¸å¯é€¾è¶Šã€‚æœ€åˆè§£é¢˜å¿ƒåˆ‡ï¼Œå¿½ç•¥æ•´ä½“æ¡†æ¶è€Œç›´æ”»å¥–åŠ±å‡½æ•°ï¼Œå¯è°ƒæ•´è¯¥å‡½æ•°åæ ¹æœ¬ä¸äº†è§£å…¶å½±å“é¢ï¼Œæœ€ç»ˆè¿˜æ˜¯èŠ±äº†å¤§é‡æ—¶é—´ç†Ÿæ‚‰è¿™ä¸ªå­¦ä¹ æ¨¡å‹çš„ä»£ç æ¡†æ¶ã€‚

# ### (å¯é€‰)Plot Actor åŠ Critic ç»“æ„
# å»ºè®®ä½¿ç”¨ ```from keras.utils import plot_model``` æ¥æ˜¾ç¤ºæ¨¡å‹ç»“æ„ï¼›

# In[116]:


from keras.utils import plot_model
from PIL import Image


print("The architecture of the actor is:")
plot_model(agent.actor_local.model, to_file='model_act.png', show_shapes=True)
plt.imshow(Image.open('model_act.png'))
plt.show()

print("\r\n\r\nThe architecture of critic is:")
plot_model(agent.critic_local.model, to_file='model_cri.png', show_shapes=True)
plt.imshow(Image.open('model_cri.png'))
plt.show()


# In[ ]:




